[m
 𜷋𜺣 𜷋𜴧𜶜𜺣𜺠𜵡𜶜𜺣 𜶜𜵡  𜶜𜵡     𜺠𜵡𜶻              𜴘▄
𜷥𜶬𜷖𜵈𜴅𜴫𜶽𜺨█  𜺨 ▐▌  ▐▌     𜷍𜶩𜷒𜴉    𜷡𜴆𜴣𜴉𜷡𜴆𜶙▖𜷋𜴔█ 𜷡𜶷𜷗▖    𜶙𜵋𜶙▖𜵸𜶷𜷞 𜷡𜴆𜷡𜴀𜷡𜶷𜷗▖𜶤𜵁𜴣𜴉
█ ▐▌𜶨▂𜷕𜴍𜴦𜶻𜷋🯦 𜷕𜷀  𜷕𜷀     𜶫▂𜶫𜺣    𜶫▂𜷋𜴉𜶫▂𜷕𜴍𜶫▂𜶫𜺣𜶫▂𜷋𜴉    ▐𜷀𜷕𜴍𜶫▂𜶫𜺣𜶫▂█ 𜶫▂𜷋𜴉𜶜𜶮𜷘𜴉
                                                    𜴱𜴬      𜴣𜴧𜴗
#6 The tumultuous journey to Unicode


 [4mIntroduction[24m

This document contains legacy computers character sets, those will display
properly only if you use a font that contains most of the Symbols for Legacy
Computing and Supplement blocks from Unicode 16.0 and many legacy characters.
I made sure that ]8;;https://github.com/slavfox/Cozette/Cozette]8;; 1.29.0 supports all of them, you can use it as it, or
as a fallback with another font of your choice for the missing characters.

This document does not cover Baudot Code, BCDIC, and EBCDIC because while
significant, they did not evolve into modern Unicode. ASCII on the other hand
evolved into Latin-1, which in turn is included as it as the first block of
Unicode. ASCII and its extensions also heavily influenced UTF-8 design.
I did not include all existing code pages, as there were plenty to support
different locales, only some notable ones to showcase the principles and
evolutions of text encoding on personal computers and cover general ideas.


The first ASCII standard from 1963 only included uppercase letters, numbers
and punctuation, as well as many control characters, and left 28 codes
unassigned.
The control characters were critical because the encoding was not just
designed to store text strings in memory or in files, but also to handle
serial communication between a time-sharing or mainframe computer and a
teletype, or other telecommunication equipments. It included in-band control
codes besides printable characters, such as move to a new line, shift to
another character set, or lower level such as handshake between devices.
Terminal emulators still use some of those control codes today, and even
plain text uses the carriage return and/or line feed codes to end lines.
While they immediately recognized that lowercase would be important as well,
they were not yet sure whether they could "waste" code that might be required
for control to provide direct access to lowercase, or if it would be better
to use some shifting scheme, such as a start-lowercase and end-lowercase pair
of codes that would change the letters to their lowercase variants.

 [4mASCII 1963 (7-bit)[24m
  [33m_0123456789ABCDEF[m
 [33m0_[90m␀␁␂␃␄␅•␇␈␉␊␋␌␍␎␏[m    ␥ = Unassigned
 [33m1_[90m␐␑␒␓␔␕␖␙••••␜␝␞␟[m    • = Control character with no modern
 [33m2_[m !"#$%&'()*+,-./        equivalent (RU, S0 to S3, and ① )
 [33m3_[m0123456789:;<=>?
 [33m4_[m@ABCDEFGHIJKLMNO
 [33m5_[mPQRSTUVWXYZ[\]↑←
 [33m6_[90m␥␥␥␥␥␥␥␥␥␥␥␥␥␥␥␥[m
 [33m7_[90m␥␥␥␥␥␥␥␥␥␥␥␥␆•␛␡[m

Many early computers didn't have enough RAM to provide a graphics mode where
individual pixels could be controlled. They were limited to displaying text.
Since most early home computers were 8-bit systems, their designers quickly
realized they could add an extra 128 characters, 0x80 to 0xFF, to provide
international alphabets and semigraphics, allowing for rudimentary graphics.

For example, the Commodore PETSCII (PET, C=64, …) was based on ASCII 1963:

 [4mPETSCII (standard)[24m    [4mPETSCII (shifted)[24m
  [33m_0123456789ABCDEF     _0123456789ABCDEF[m
 [33m0_[100;30;3mPETSCII-specific[m    [33m0_[100;30;3mPETSCII-specific[m
 [33m1_[100;30;3m control chars  [m    [33m1_[100;30;3m control chars  [m
 [33m2_[90m !"#$%&'()*+,-./    [33m2_[90m !"#$%&'()*+,-./[m
 [33m3_[90m0123456789:;<=>?    [33m3_[90m0123456789:;<=>?[m
 [33m4_[90m@ABCDEFGHIJKLMNO    [33m4_[90m@[mabcdefghijklmno
 [33m5_[90mPQRSTUVWXYZ[[m£[90m]↑←    [33m5_[mpqrstuvwxyz[90m[£]↑←[m
 [33m6_[m🭹♠🭲🭸🭷🭶🭺🭱🭴╮╰╯🭼╲╱🭽    [33m6_[90m🭹[mABCDEFGHIJKLMNO
 [33m7_[m🭾●🭻♥🭰╭╳○♣🭵♦┼🮌│π◥    [33m7_[mPQRSTUVWXYZ[90m┼🮌│[m🮖🮘
 [33m8_[100;30;3mRepeat of 00-1F [m    [33m8_[100;30;3mRepeat of 00-1F [m
 [33m9_[100;30;3m control chars  [m    [33m9_[100;30;3m control chars  [m
 [33mA_[m ▌▄▔▁▏🮕▕🮏◤🮇├▗└┐▂    [33mA_[90m ▌▄▔▁▏🮕▕🮏[m🮙[90m🮇├▗└┐▂[m
 [33mB_[m┌┴┬┤▎▍🮈🮂🮃▃🭿▖▝┘▘▚    [33mB_[90m┌┴┬┤▎▍🮈🮂🮃▃[m✓[90m▖▝┘▘▚[m
 [33mC_[100;30;3mRepeat of 40-5F [m    [33mC_[100;30;3mRepeat of 40-5F [m
 [33mD_[100;30;3m letters+symbols[m    [33mD_[100;30;3m letters+symbols[m
 [33mE_[100;30;3mRepeat of A0-BF [m    [33mE_[100;30;3mRepeat of A0-BF [m
 [33mF_[100;30;3m semigraphics   [m    [33mF_[100;30;3m semigraphics   [m

The committee in charge of ASCII quickly decided that lowercase letters were
important to include as well, and deprecated some control characters and
moved the remaining ones into the 0x00 to 0x1F range, now referred to as C0
control characters, to leave the 0x60 to 0x7F available for a lowercase copy
of the 0x40 to 0x5F range. This made it easy to change from uppercase to
lowercase or the other way around with a single bit change, and perform
case-insensitive comparisons by ignoring (masking out) one bit.
While this was only officially published as the standard in 1967, the
decision and draft came very soon after publishing the 1963 standard, so
many early computers integrated the changes or part of them quickly.

 [4mASCII 1967 (7-bit)[24m
  [33m_0123456789ABCDEF[m
 [33m0_[90m␀␁␂␃␄␅␆␇␈␉␊␋␌␍␎␏[m
 [33m1_[90m␐␑␒␓␔␕␖␗␘␙␚␛␜␝␞␟[m
 [33m2_[90m !"#$%&'()*+,-./[m
 [33m3_[90m0123456789:;<=>?[m
 [33m4_[90m@ABCDEFGHIJKLMNO[m
 [33m5_[90mPQRSTUVWXYZ[\][m^_
 [33m6_[m`abcdefghijklmno
 [33m7_[mpqrstuvwxyz{|}~[90m␡[m

The Apple II for example decided to base its characters set on this revision,
providing ^ and _ instead of ↑ and ←. But to save ROM space, they only
included the original uppercase glyphs, making the sequence repeat with
different visual effects while only storing 32 8×8 glyphs (256 bytes).

 [4mApple II[24m
  [33m_0123456789ABCDEF[m
 [33m0_[0;7m@ABCDEFGHIJKLMNO[m ╶╮
 [33m1_[0;7mPQRSTUVWXYZ[\]^_[m  ├╴ Inverse
 [33m2_[0;7m !"#$%&'()*+,-./[m  │  (reversed video / negative)
 [33m3_[0;7m0123456789:;<=>?[m ╶╯
 [33m4_[39;47m@ABCDEFGHIJKLMNO[m ╶╮
 [33m5_[39;47mPQRSTUVWXYZ[\]^_[m  ├╴ Blinking (alternating inverse)
 [33m6_[39;47m !"#$%&'()*+,-./[m  │  (Changed to MouseText in Apple IIc and
 [33m7_[39;47m0123456789:;<=>?[m ╶╯  IIGS, and Enhanced Apple IIe upgrade.)
 [33m8_[m@ABCDEFGHIJKLMNO ╶╮
 [33m9_[mPQRSTUVWXYZ[\]^_  ├╴ Normal
 [33mA_[m !"#$%&'()*+,-./  │
 [33mB_[m0123456789:;<=>? ╶╯
 [33mC_[37m@ABCDEFGHIJKLMNO[m ╶╮
 [33mD_[37mPQRSTUVWXYZ[\]^_[m  ├╴ Repeat of normal
 [33mE_[37m !"#$%&'()*+,-./[m  │
 [33mF_[37m0123456789:;<=>?[m ╶╯

This means it was not really compatible with ASCII, as the 0x20 to 0x3F rows
originally assigned to punctuation and digits would be their inverse
versions, the uppercase letters 0x40 to 0x5F would be their blinking
versions, and the lowercase letters 0x60 to 0x7F would be the blinking
punctuation and digits instead.
This is part of a "based on ASCII but not quite ASCII" rough start
developers on early systems encountered.

Some manufacturers also realized they could add glyphs to the C0 control
characters and delete. They would be interpreted as in-band control when
printed or transmitted, but if that value was stored in video memory for a
character of the text screen, it would render as semigraphics, providing 17
more glyphs.

 [4mTRS-80[24m                [4mMattel Aquarius[24m       [4mAmstrad CPC[24m
  [33m_0123456789ABCDEF     _0123456789ABCDEF     _0123456789ABCDEF[m
 [33m0_[m␨£|éÜÅ¬öØùñ`āŔÄÃ    [33m0_[m£½¼¾÷©→←↑↓↗↙↘↖𜷰𜷱    [33m0_[m◻⎾⏊⏌🗲⊠✓⍾←→↓↑↡↲⊗⊙
 [33m1_[mÑÖØÕßüõæäàȧ∮ÉÆÇ˜    [33m1_[m𜷲𜷳𜷴𜷦𜷧𜷨𜷩𜷪𜷫𜷵▗▝▖▘▚▄    [33m1_[m⊟◷◶◵◴⍻⎍⊣⧖⍿␦⊖◰◱◲◳
 [33m2_[90m !"#$%&'()*+,-./    [33m2_[90m !"#$%&'()*+,-./[m    [33m2_[90m !"#$%&'()*+,-./[m
 [33m3_[90m0123456789:;<=>?    [33m3_[90m0123456789:;<=>?[m    [33m3_[90m0123456789:;<=>?[m
 [33m4_[90m@ABCDEFGHIJKLMNO    [33m4_[90m@ABCDEFGHIJKLMNO[m    [33m4_[90m@ABCDEFGHIJKLMNO[m
 [33m5_[90mPQRSTUVWXYZ[\]^_    [33m5_[90mPQRSTUVWXYZ[\]^_[m    [33m5_[90mPQRSTUVWXYZ[\][m↑[90m_[m
 [33m6_[90m`abcdefghijklmno    [33m6_[90m`abcdefghijklmno[m    [33m6_[90m`abcdefghijklmnp[m
 [33m7_[90mpqrstuvwxyz{[m¦[90m}~[m±    [33m7_[90mpqrstuvwxyz{[m¦[90m}~[m█    [33m7_[90mpqrstuvwxyz{|}~[m␩
 [33m8_[m 🬀🬁🬂🬃🬄🬅🬆🬇🬈🬉🬊🬋🬌🬍🬎    [33m8_[m▇▏𜷬𜷭🮏🮌▒●▂▆🛧𜷸𜷮𜷯▶▲    [33m8_[m ▘▝▀▖▌▞▛▗▚▐▜▄▙▟█
 [33m9_[m🬏🬐🬑🬒🬓▌🬔🬕🬖🬗🬘🬙🬚🬛🬜🬝    [33m9_[m▁▉𜷶𜷷🮎🮍⬤▎▍▌✈𜷹𜷼𜷻◀▼    [33m9_[m·╵╶╰╷│╭├╴╯─┴╮┤┬┼
 [33mA_[m🬞🬟🬠🬡🬢🬣🬤🬥🬦🬧▐🬨🬩🬪🬫🬬    [33mA_[m 🬀🬁🬂🬃🬄🬅🬆🬇🬈🬉🬊🬋🬌🬍🬎    [33mA_[m^´¨£©¶§‘¼½¾±÷¬¿¡
 [33mB_[m🬭🬮🬯🬰🬱🬲🬳🬴🬵🬶🬷🬸🬹🬺🬻█    [33mB_[m🬏🬐🬑🬒🬓▌🬔🬕🬖🬗🬘🬙🬚🬛🬜🬝    [33mB_[mαβγδεθλμπσφψχωΣΩ
 [33mC_[m♠♥♦♣☺☹≤≥αβγδεζηθ    [33mC_[m◢◣🯨▊▪♦·🯬┼🯭╱🯩┴├┐└    [33mC_[m🮠🮡🮣🮢🮧🮥🮦🮤🮨🮩🮮╳╱╲🮕▒
 [33mD_[mικλμνξοπρστυφχψω    [33mD_[m𜷿𜷾🯪𜷽♥♣│🯮╳🯯╲🯫┬┤┌┘    [33mD_[m▔▕▁▏◤◥◢◣🮎🮍🮏🮌🮜🮝🮞🮟
 [33mE_[mΩ√÷∑≈∆⌇≠🗲%⍾∞✓§🯋©    [33mE_[m🬞🬟🬠🬡🬢🬣🬤🬥🬦🬧▐🬨🬩🬪🬫🬬    [33mE_[m☺☹♣♦♥♠○●□■♂♀♩♪☼𜱗
 [33mF_[m¤¶¢®🯁🯂🯃℞℅♂♀🯌🯄🯅🯉🯊    [33mF_[m🬭🬮🬯🬰🬱🬲🬳🬴🬵🬶🬷🬸🬹🬺🬻█    [33mF_[m⭡⭣⭠⭢▲▼▶◀🯆🯅🯇🯈𜱣𜱤⭥⭤

The TRS-80 and Mattel Aquarius lack a graphics mode, instead they compensate
for it with sets of characters that provide a low-resolution set of 2×2 or
2×3 pseudo-pixels mosaics patterns, making it possible to create crude
arbitrary graphics.

Some character sets, such as the Amstrad CPC above, kept the ↑ symbols from
ASCII 1963 in place of ^ from ASCII 1967. These were pretty much considered
equivalent in the 1980's, and explains why most flavors of the BASIC language
use ^ as the exponentiation operator, which made perfect sense as ↑.

When IBM introduced the IBM 5150 Personal Computer in 1981, they decided to
follow the trend by using ASCII and adding extra characters both in 0x80 to
0xFF and C0 (0x00 to 0x1F). However, being "Business Machines", they focused
on supporting extra alphabets (for other languages and math), and
semigraphics to draw tables, because businesses need products inventories and
tabular data more than game sprites and tiles.

 [4mIBM PC[24m
  [33m_0123456789ABCDEF[m
 [33m0_[m ☺☻♥♦♣♠•◘○◙♂♀♪♫☼
 [33m1_[m►◄↕‼¶§▬↨↑↓→←∟↔▲▼
 [33m2_[90m !"#$%&'()*+,-./[m
 [33m3_[90m0123456789:;<=>?[m
 [33m4_[90m@ABCDEFGHIJKLMNO[m
 [33m5_[90mPQRSTUVWXYZ[\]^_[m
 [33m6_[90m`abcdefghijklmno[m
 [33m7_[90mpqrstuvwxyz{[m¦[90m}~[m⌂

While the low half is based on ASCII 1967 and identical in all markets
(except for DOS/V), the original US-centric high half designed for the US
didn't support other western countries. It was redesigned for each target
market and lost some box drawing pieces to allow for more alphabetic letters
to accomodate specific regions and languages.

 [4mDOS 437: IBM-PC/US[24m    [4mDOS 850: Latin 1[24m
  [33m_0123456789ABCDEF     _0123456789ABCDEF[m
 [33m8_[mÇüéâäàåçêëèïîìÄÅ    [33m8_[90mÇüéâäàåçêëèïîìÄÅ[m
 [33m9_[mÉæÆôöòûùÿÖÜ¢£¥₧ƒ    [33m9_[90mÉæÆôöòûùÿÖÜ[mø[90m£[mØ×[90mƒ[m
 [33mA_[máíóúñÑªº¿⌐¬½¼¡«»    [33mA_[90máíóúñÑªº¿[m®[90m¬½¼¡«»[m
 [33mB_[m░▒▓│┤╡╢╖╕╣║╗╝╜╛┐    [33mB_[90m░▒▓│┤[mÁÂÀ©[90m╣║╗╝[m¢¥[90m┐[m
 [33mC_[m└┴┬├─┼╞╟╚╔╩╦╠═╬╧    [33mC_[90m└┴┬├─┼[mãÃ[90m╚╔╩╦╠═╬[m¤
 [33mD_[m╨╤╥╙╘╒╓╫╪┘┌█▄▌▐▀    [33mD_[mðÐÊËÈıÍÎÏ[90m┘┌█▄[m¦Ì[90m▀[m
 [33mE_[mαßΓπΣσµτΦΘΩδ∞φε∩    [33mE_[mÓ[90mß[mÔÒõÕ[90mµ[mþÞÚÛÙýÝ¯´
 [33mF_[m≡±≥≤⌠⌡÷≈°∙·√ⁿ²■     [33mF_[m­[90m±[m‗¾¶§[90m÷[m¸[90m°[m¨[90m·[m¹³[90m²■[m

 [4mDOS 860: Portuguese[24m   [4mDOS 863: fr-Canada[24m    [4mDOS 865: Nordic[24m
  [33m_0123456789ABCDEF     _0123456789ABCDEF     _0123456789ABCDEF[m
 [33m8_[90mÇüéâ[mã[90mà[mÁ[90mçê[mÊ[90mè[mÍÔ[90mì[mÃÂ    [33m8_[90mÇüéâ[mÂ[90mà[m¶[90mçêëèïî[m‗À§    [33m8_[90mÇüéâäàåçêëèïîìÄÅ[m
 [33m9_[90mÉ[mÀÈ[90mô[mõ[90mò[mÚ[90mù[mÌÕ[90mÜ¢£[mÙ[90m₧[mÓ    [33m9_[90mÉ[mÈÊ[90mô[mËÏ[90mûù[m¤Ô[90mÜ¢£[mÙÛ[90mƒ    [33m9_[90mÉæÆôöòûùÿÖÜ[mø[90m£[mØ[90m₧ƒ[m
 [33mA_[90máíóúñÑªº¿[mÒ[90m¬½¼¡«»    [33mA_[m¦´[90móú[m¨¸³¯Î[90m⌐¬½¼[m¾[90m«»    [33mA_[90máíóúñÑªº¿⌐¬½¼¡«[m¤
 [33mB_[90m░▒▓│┤╡╢╖╕╣║╗╝╜╛┐    [33mB_[90m░▒▓│┤╡╢╖╕╣║╗╝╜╛┐    [33mB_[90m░▒▓│┤╡╢╖╕╣║╗╝╜╛┐[m
 [33mC_[90m└┴┬├─┼╞╟╚╔╩╦╠═╬╧    [33mC_[90m└┴┬├─┼╞╟╚╔╩╦╠═╬╧    [33mC_[90m└┴┬├─┼╞╟╚╔╩╦╠═╬╧[m
 [33mD_[90m╨╤╥╙╘╒╓╫╪┘┌█▄▌▐▀    [33mD_[90m╨╤╥╙╘╒╓╫╪┘┌█▄▌▐▀    [33mD_[90m╨╤╥╙╘╒╓╫╪┘┌█▄▌▐▀[m
 [33mE_[90mαßΓπΣσµτΦΘΩδ∞φε∩    [33mE_[90mαßΓπΣσµτΦΘΩδ∞φε∩    [33mE_[90mαßΓπΣσµτΦΘΩδ∞φε∩[m
 [33mF_[90m≡±≥≤⌠⌡÷≈°∙·√ⁿ²■     [33mF_[90m≡±≥≤⌠⌡÷≈°∙·√ⁿ²■     [33mF_[90m≡±≥≤⌠⌡÷≈°∙·√ⁿ²■ [m

The Japanese Industrial Standard JIS X 0201, developed in 1969, was similar
to how western manufacturers extended ASCII 1967 to add the characters
required for specific languages. Since Japanese could write and understand
everything written in latin alphanumerics and Katakana, a Japanese phonetic
alphabet usually reserved to foreign words, but that could best be
represented in the low 8×8 resolution of most early computers.

 [4mJIS X 0201[24m
  [33m_0123456789ABCDEF[m
 [33m0_[100;30;3m   C0 control   [m
 [33m1_[100;30;3m   characters   [m
 [33m2_[90m !"#$%&'()*+,-./[m
 [33m3_[90m0123456789:;<=>?[m
 [33m4_[90m@ABCDEFGHIJKLMNO[m
 [33m5_[90mPQRSTUVWXYZ[[m¥[90m]^_[m
 [33m6_[90m`abcdefghijklmno[m
 [33m7_[90mpqrstuvwxyz{|}[m‾[90m␡[m
 [33m8_[100;30;3m   C1 control   [m
 [33m9_[100;30;3m   characters   [m
 [33mA_[100;30m␥[m｡｢｣､･ｦｧｨｩｪｫｬｭｮｯ
 [33mB_[mｰｱｲｳｴｵｶｷｸｹｺｻｼｽｾｿ
 [33mC_[mﾀﾁﾂﾃﾄﾅﾆﾇﾈﾉﾊﾋﾌﾍﾎﾏ
 [33mD_[mﾐﾑﾒﾓﾔﾕﾖﾗﾘﾙﾚﾛﾜﾝﾞﾟ
 [33mE_[100;30m␥␥␥␥␥␥␥␥␥␥␥␥␥␥␥␥[m
 [33mF_[100;30m␥␥␥␥␥␥␥␥␥␥␥␥␥␥␥␥[m

I won't cover the details of the Hitachi Basic Master MB-6880, Sharp MZ-80K,
and NEC PC-8001 and their successors, because while significants in Japan,
these systems did not influence other character sets outside of their series.
The only notable legacy of these systems in modern Unicode is the inclusion
of their semigraphic characters not found on other systems as part of the
Symbols for Legacy Computing Supplement blocks in Unicode 16.0.
It can however be noted that while these systems were based on ASCII and
Katakana, Sharp and NEC included some Kanji used to write dates as part of
their semigraphics, which probably influenced the MSX character set design.

For the MSX system, Microsoft based its international character set on the
IBM PC / DOS 437, but moving the single-line box drawing characters to the
0x1x row, and including more original semigraphics in the 0xC0 to 0xDF space
instead. For the Japanese version, they extended the JIS standard to add
Hiragana, the other Japanese phonetic alphabet, and 18 Kanji, which only left
space for a few semigraphics characters. They will have to wait for some
hardware improvements before they will be able to support more Kanji.

 [4mMSX International[24m     [4mMSX Japanese[24m (based on JIS X 0201)
  [33m_0123456789ABCDEF     _０１２３４５６７８９ＡＢＣＤＥＦ[m
 [33m0_[90m ☺☻♥♦♣♠•◘○◙♂♀♪♫☼    [33m0_[m　月火水木金土日年円時分秒百千万
 [33m1_[m⟊┴┬┤├┼│─┌┐└┘╳╱╲🮯    [33m1_[mπ ┴ ┬ ┤ ├ ┼ │ ─ ┌ ┐ └ ┘ ╳ 大中小
 [33m2_[90m !"#$%&'()*+,-./    [33m2_[90m　！＂＃＄％＆＇（）＊＋，－．／[m
 [33m3_[90m0123456789:;<=>?    [33m3_[90m０１２３４５６７８９：；＜＝＞？[m
 [33m4_[90m@ABCDEFGHIJKLMNO    [33m4_[90m＠ＡＢＣＤＥＦＧＨＩＪＫＬＭＮＯ[m
 [33m5_[90mPQRSTUVWXYZ[\]^_    [33m5_[90mＰＱＲＳＴＵＶＷＸＹＺ［[m￥[90m］＾＿[m
 [33m6_[90m`abcdefghijklmno    [33m6_[90m｀ａｂｃｄｅｆｇｈｉｊｋｌｍｎｏ[m
 [33m7_[90mpqrstuvwxyz{¦}~␡    [33m7_[90mｐｑｒｓｔｕｖｗｘｙｚ｛[m￤[90m｝[m～[90m ␡[m
 [33m8_[90mÇüéâäàåçêëèïîìÄÅ    [33m8_[m♠ ♥ ♣ ♦ ○ ● をぁぃぅぇぉゃゅょっ
 [33m9_[90mÉæÆôöòûùÿÖÜ¢£¥₧ƒ    [33m9_[100;30m␥ [mあいうえおかきくけこさしすせそ
 [33mA_[90máíóúñÑªº¿⌐¬½¼¡«»    [33mA_[100;30m␥ [0;90m。「」、・ヲァィゥェォャュョッ[m
 [33mB_[mÃãĨĩÕõŨũĲĳ¾∽◊‰¶§    [33mB_[90mーアイウエオカキクケコサシスセソ[m
 [33mC_[m▂▚▆🮂▬🮅▎▞▊🮇🮊🮙🮘🭭🭯🭬    [33mC_[90mタチツテトナニヌネノハヒフヘホマ[m
 [33mD_[m🭮🮚🮛▘▗▝▖🮖Δ‡ω[90m█▄▌▐▀    [33mD_[90mミムメモヤユヨラリルレロワン゙゚[m
 [33mE_[90mαßΓπΣσµτΦΘΩδ∞⌀∈∩    [33mE_[mたちつてとなにぬねのはひふへほま
 [33mF_[90m≡±≥≤⌠⌡÷≈°∙·√ⁿ²■█    [33mF_[mみむめもやゆよらりるれろわん[100;30m␥ [0;90m██[m

All these code pages were still 8-bit each. A system like MS-DOS could change
the code page dynamically by loading another character set into memory, but
an app would be limited to a single character set of 256 glyphs at most.

This still would not allow to mix languages on the same screen. Some users
needed more symbols to represent mathematical concepts, or mix languages.
Some terminals such as DEC and HP allowed the user to install several
characters ROMs, and switch between them using in-band commands (VT escape
sequences). All the ROMs would contain the same numerical values for their
characters, so plain text could not mix them, but the terminal would keep
track of the character set in use and store the ROM index along the character
value for each cell of the screen, allowing a single screen to mix characters
from several ROMs. DEC VT terminals even allowed apps to load custom
characters in RAM, making it possible to define their own alternate character
sets.

For example, here are the character ROMs for the HP 2640 Series terminals.
It stored 64 characters per ROM, so the base characters based on ASCII 1967
was split over two ROMs, and alternate character set ROMs would only contain
alternates for the characters 0x20 to 0x5F.

 [4mRoman Uppercase[24m       [4mMath character set[24m    [4mLine Microvector set[24m
  [33m_0123456789ABCDEF     _0123456789ABCDEF     _0123456789ABCDEF[m
 [33m2_[90m !"#$%&'()*+,-./[m    [33m2_[m √│§∇±∝⌠÷≃ΠΓΨ≡ΦΞ    [33m2_[m ┠┨┯┷╟╢╤╧║╂┿─𜸗│┼
 [33m3_[90m0123456789:;<=>?[m    [33m3_[m⁰¹²³⁴⁵⁶⁷⁸⁹ΩΛ∞⌡☨Σ    [33m3_[m╋┣┫┳┻├┤┬┴═┃━╫𜸖╪╬
 [33m4_[90m@ABCDEFGHIJKLMNO[m    [33m4_[m¶αβψφε∂ληιΘκωμνρ    [33m4_[m╞┗╉█🯎🯏└┘𜸏𜸔𜸐𜸑𜸒╇╈𜸕
 [33m5_[90mPQRSTUVWXYZ[\]^_[m    [33m5_[mπγθστξΔδχυζ↑→Υ←↓    [33m5_[m𜸙┏┌┛┐┤╊┓▅𜸘▃╡𜸍╨𜸎╥

 [4mRoman Lowercase[24m      [4mLarge Character set[24m   The large character was
  [33m_0123456789ABCDEF     _0123456789ABCDEF[m   designed to build large
 [33m0_[90m␀␁␂␃␄␅␆[m🕭[90m␈␉␊␋␌␍␎␏[m    [33m2_[m 𜸚𜸛𜸜𜸝𜸞𜸟𜸠𜸡𜸢𜸣𜸤𜸥𜸦𜸧𜸨   characters like these:
 [33m1_[90m␐␑␒␓␔␕␖␗␘␙␚␛␜␝␞␟[m    [33m3_[m𜸩𜸪𜸫𜸬𜸭𜸮𜸯𜸰𜸫𜸱𜸲𜸳𜸴▚𜸵𜸶   𜸚𜸟𜸤𜸛𜸟𜸤𜸚𜸟𜸤 𜸦 𜸚𜸟𜸤𜸚𜸟𜸤
 [33m6_[90m`abcdefghijklmno[m    [33m4_[m𜸷𜸸𜸹𜸺𜸻𜸼𜸽𜸾𜸿𜹀𜹁𜹂𜹃𜹄𜹅𜹆   𜸨𜸟𜸶𜸨𜸟𜸷𜸩   𜸩 𜸚𜸟𜹃 𜸟𜸷
 [33m7_[90mpqrstuvwxyz{|}~[m␨    [33m5_[m𜹇𜹈𜹉𜹊𜹋𜹌𜹍𜹎𜹏𜹐▘▝▖▗▀▌   𜸼 𜸼𜸽𜸟𜹃𜸾𜸟𜹃 𜸼 𜸽𜸟𜸥𜸾𜸟𜹃

They also supported purpose-specific character ROMs, such as 3 ROMs for APL.

When systems capable of graphics appeared, semigraphics became useless.
It was much better to draw tables and game graphics using the graphics modes
and to use the text encoding to represent as many characters as possible.
Code pages kept the ASCII standard and defined as many punctuation and
letters as possible as the remaining 128 codes.

The ISO Latin 1 character set is designed to handle most western languages,
and Microsoft extended it, adding some missing punctuation and letters in
place of the C1 control characters, as was done earlier with the C0 control
characters. These two code pages are very closely linked, so much that Web
pages that claim to be ISO Latin 1 are in fact assumed to be Windows 1252.

 [4mISO 8859-1:Latin 1[24m    [4mWin 1252: Latin 1[24m
  [33m_0123456789ABCDEF     _0123456789ABCDEF[m
 [33m8_[100;30;3m   C1 control   [m    [33m8_[m€[100;30m␥[m‚ƒ„…†‡ˆ‰Š‹Œ[100;30m␥[mŽ[100;30m␥[m
 [33m9_[100;30;3m   characters   [m    [33m9_[100;30m␥[m‘’“”•–—˜™š›œ[100;30m␥[mžŸ
 [33mA_[m ¡¢£¤¥¦§¨©ª«¬­®¯    [33mA_[90m ¡¢£¤¥¦§¨©ª«¬­®¯[m
 [33mB_[m°±²³´µ¶·¸¹º»¼½¾¿    [33mB_[90m°±²³´µ¶·¸¹º»¼½¾¿[m
 [33mC_[mÀÁÂÃÄÅÆÇÈÉÊËÌÍÎÏ    [33mC_[90mÀÁÂÃÄÅÆÇÈÉÊËÌÍÎÏ[m
 [33mD_[mÐÑÒÓÔÕÖ×ØÙÚÛÜÝÞß    [33mD_[90mÐÑÒÓÔÕÖ×ØÙÚÛÜÝÞß[m
 [33mE_[màáâãäåæçèéêëìíîï    [33mE_[90màáâãäåæçèéêëìíîï[m
 [33mF_[mðñòóôõö÷øùúûüýþÿ    [33mF_[90mðñòóôõö÷øùúûüýþÿ[m

On early Windows, before Unicode, both philosophies were in use
simultaneously, the "OEM" code pages for text-mode (CUI) applications with
their semigraphics from the IBM PC and MS-DOS days, and "ANSI" code pages for
GUI applications.

Just like some terminals supported displaying several character sets on a
single screen, GUI systems usually allowed apps to select the font to use
for each text draw call. This allowed to mix languages on screen, or even
design fonts specifically to provide symbols to be used alongside normal
text. For example, the Windows Wingdings fonts simply reuse the same 224
printable character values to privide symbols. While this worked, it still
required the app to carefully select its fonts, and these alternate character
sets would not work for text interchange as the characters from different
sets used the same values.

This was not enough for the asian market, and Japan with leading hardware
manufacturers wanted to support their local market properly.
But how could that be achieved when you want to support thousands of
ideographs?

Well, you need more bits to represent them, and since the machines are 8-bit,
the easiest solution is to extend to two bytes, providing 16 bits, which
could support 65536 glyphs. Easy enough, but then you are not compatible with
ASCII anymore and break compatibility with every western application.
There has to be a better and clever way to have it both ways.

The solution was to mix 8-bit and 16-bit in a single encoding, called
Multibyte Character Sets (MBCS). The way it works is to keep the JIS X 0201
characters as they are, and only use some of the unused codes as leading
bytes of double-byte characters (DBCS). This reduces the total number of
glyphs that the encoding system can represent, but can easily support all
common and less common ideographs, even include Greek, Cyrillic, and extended
Mathematical symbols for technical documents, and stay compatible with ASCII
systems. This also makes decoding it much harder, as it has to be read from
the beginning to find out if a value that is valid as a leading byte is a
leading byte and not a trailing byte.
A variable-width encoding also makes it much more difficult to find out how
many characters are in a text string. Fortunately in its original
implementation in Japanese DOS/V, all the double-byte characters were also
double-width (called "full-width", relegating the normal width to
"half-width"). This was also convenient to handle text video memory, as it
could stay a fixed grid of characters, and double-byte characters would
simply store their values using two adjacent cells of the text video screen.
Each DBCS leading byte provides a whole plane of characters.

 [4mDOS 932: Shift JIS[24m
  [33m_0123456789ABCDEF[m
 [33m0_[90m␀[m╔╗╚╝║═￬[90m␈[m￮[90m␊[m〿[90m␌␍[m￭☼
 [33m1_[m╬[90m␑[m↕[90m␓[m▓╩╦╣[90m␘[m╠░↵￪￨￫￩
 [33m2_[90m !"#$%&'()*+,-./[m   ╭─→ [4mE8: JIS X 0208 Kanji Level 2 Row 79-80[24m
 [33m3_[90m0123456789:;<=>?[m   │    [33m_０１２３４５６７８９ＡＢＣＤＥＦ[m
 [33m4_[90m@ABCDEFGHIJKLMNO[m   │   [33m4_[m錙錢錚錣錺錵錻鍜鍠鍼鍮鍖鎰鎬鎭鎔
 [33m5_[90mPQRSTUVWXYZ[[m¥[90m]^_[m   │   [33m5_[m鎹鏖鏗鏨鏥鏘鏃鏝鏐鏈鏤鐚鐔鐓鐃鐇
 [33m6_[90m`abcdefghijklmno[m   │   [33m6_[m鐐鐶鐫鐵鐡鐺鑁鑒鑄鑛鑠鑢鑞鑪鈩鑰
 [33m7_[90mpqrstuvwxyz{|}[m‾[90m␡[m   │   [33m7_[m鑵鑷鑽鑚鑼鑾钁鑿閂閇閊閔閖閘閙
 [33m8_[100;30m␥[42;39;3m     DBCS      [m   │   [33m8_[m閠閨閧閭閼閻閹閾闊濶闃闍闌闕闔闖
 [33m9_[42;39;3m  leading bytes [m   │   [33m9_[m關闡闥闢阡阨阮阯陂陌陏陋陷陜陞陝
 [33mA_[100;30m␥[0;90m｡｢｣､･ｦｧｨｩｪｫｬｭｮｯ[m   │   [33mA_[m陟陦陲陬隍隘隕隗險隧隱隲隰隴隶隸
 [33mB_[90mｰｱｲｳｴｵｶｷｸｹｺｻｼｽｾｿ[m   │   [33mB_[m隹雎雋雉雍襍雜霍雕雹霄霆霈霓霎霑
 [33mC_[90mﾀﾁﾂﾃﾄﾅﾆﾇﾈﾉﾊﾋﾌﾍﾎﾏ[m   │   [33mC_[m霏霖霙霤霪霰霹霽霾靄靆靈靂靉靜靠
 [33mD_[90mﾐﾑﾒﾓﾔﾕﾖﾗﾘﾙﾚﾛﾜﾝﾞﾟ[m   │   [33mD_[m靤靦靨勒靫靱靹鞅靼鞁靺鞆鞋鞏鞐鞜
 [33mE_[42;39m        ×╶──────[m───╯   [33mE_[m鞨鞦鞣鞳鞴韃韆韈韋韜韭齏韲竟韶韵
 [33mF_[42;39m             [100;30m␥␥␥[m       [33mF_[m頏頌頸頤頡頷頽顆顏顋顫顯顰

This arrangement allowed for an MBCS system to contain the 6879 glyphs
standardized by JIS X 0208, and even add some NEC and IBM extras, and 189
End-User-Defined Characters (EUDC) to allow users and apps to provide extra
glyphs, similarly to Private Use Area characters in Unicode today.
This required a PC with a VGA display adapter and some extended memory to
store the 258 KB of bitmap glyphs required, or more for EUDC glyphs.
PC systems in Japan are still sometimes referred to as DOS/V systems because
of this achievement that made personal computers much more usable for them.

Code pages for Korean and Chinese used the same technique to support a large
number of characters needed for their respective writing systems.

In those times, loading a plain-text file coming from another country would
usually give incorrect extended ASCII characters, as the same codes were
assigned to different characters depending on the code page in use.
Encoding-savvy users could change the code page before loading the file if
they knew which code page has been used to write the text, but even that was
not always possible. On MS-DOS, MBCS encoding used for Japanese required
specific extensions only available in DOS/V, so you could never just change
the code page to 932 on a western computer.

This was just accepted as a fact of countries and locales differences, and
after all, how often do you really get a floppy disk from another continent
that contained a text file you couldn't even display properly?
There was this group of developers from different companies who dreamt of
unifying all the codes into a single universal system, which would greatly
improve scientific, linguistic, and history documents that needed to mix
writing systems, but why would common users ever want to give up the
simplicity of 8-bit text encoding to display so many characters?
But then, the Internet happened!

 [4mUnicode[24m

The principle of Unicode is very simple, we use 16-bit instead of 8-bit, and
everything works like it did for the previous 8-bit encoding system. We can
count the number of characters by counting the number of 16-bit codes, we can
directly access any character by its index into the text string, etc... it's
simpler than MBCS, at the expense of using twice as much space for the
Latin 1 characters, but that's an acceptable price to pay to unify the whole
world.
This is now called UCS-2, for 2-byte Universal Character Set, but originally
it was simply called "Unicode", as that was the only encoding we will ever
need, right?
Manufacturers were happy to jump on the standard to finally simplify
localization. Operating Systems such as Windows NT, and languages like Java,
JavaScript, and many others decided to use this encoding as their standard.

The main concept of a universal character set is simple, take a larger type
of numbers so you can store more individual characters, and put all those
code pages into a single table. Since almost all computers used 8-bit chunks
of memory, the logical choice was 16 bits, two bytes. After all, 65536
characters ought to be enough for everyone.
The idea was that every character ever used could be added to the list,
trying to keep related characters together in blocks, and every existing
code page could be converted to that system and back.
The idea of keeping the 7-bit ASCII as it is made it into Unicode, so the
first 127 characters are the ASCII 1967. And while we're at it, most western
countries are using ISO Latin 1 (8859-1) as their extended ASCII code page,
so it would make it very simple to convert to and from that code page to
simply keep those as the 256 first characters. Ok, we've still got 65280 code
points available, plenty to support all the alphabets and writing systems of
the world. This is great, we can add everything!
So next we add the less common latin letters that didn't make it into ISO
Latin 1, Greek, Cyryllic, Armenian, Hebrew, Arabic, … All the ideographs used
in Japan, the JIS X 0208 evolved into JIS X 0212 since the DOS/V days and now
contains 5801 kanji neatly organized. For China, the GB 12345 standard
contains 6866 ideographs, the Korean KS X 1001 standard contains 4620 hanja
(ideographs), as well as 11172 Hangul syllables. Taiwan uses traditional
ideographs, so they should be separate characters from their Chinese
equivalents, … Ok, let's not take a third of our 16-bit codes just for those,
we can probably be smart and merge them into a unified set to keep all those
ideographs under control. These will be the CJK (Chinese-Japanese-Korean)
Unified Ideographs blocks.

We can also save characters by avoiding all the accents and diacritics
variations of the same letter. Instead of having ten different lowercase "a",
and ten different lowercase "i", and "o", etc…, we could just have each
letter and each accent that can combine together and display as a single
glyph. This will make processing text strings more difficult since it means
some codes are not printable characters by themselves anymore, and the
perceived length of a string is not the same as the number of codes it
contains, but that isn't a big issue in the end, many fonts are not
monospaced, and perceived characters can even depend on culture, so we'll
work it out with text libraries.

Now we have enough codes left to include other languages with special writing
systems, such as Thai, Tibetan, Cherokee, … no language should be left out of
a universal characters system. After all, if this becomes the standard, a
language that cannot be used on a computer will be in trouble, so everything
should be supported. And for historians and academic papers, we need to
include all the older languages that are not in use anymore but that we want
to document. So that's a thousand cuneiform characters, a thousand Egyptian
hieroglyphs, oh, and over 40 thousands historic ideographs that are not used
nowadays but should be included if we want to encode historical documents.

Let's go crazy and add everything we can find printed somewhere, even if
we're not quite sure some of them are really characters or errors in the copy
process or even ink stains! After all, we still have thousands of codes
available!
Oh! Actually, we're running out of 16-bit values already! Who could have seen
this coming?!
We'll need to add more bits, memory got cheap and we could easily move to a
32-bit value, UCS-4 replacing UCS-2, and have enough for eternity.
But can we really deprecate years of Windows NT, Java, JavaScript, C/C++, etc
code that got on the bandwagon early?
We need to find a way to extend the number of characters while remaining
compatible with UCS-2 systems, keeping existing codes as they are, but using
the remaining ones to represent more characters… Some kind of variable-length
encoding where a block of UCS-2 codes would be a leading value of a pair,
allowing for new planes of characters. Giving a weird feeling of déjà-vu.

So Unicode deprecated UCS-2, but assigned blocks that UCS-2 could still
represent to be used as leading and trailing values of 32-bit pairs.
This kept the existing characters as they were, and enabled a million more
characters. The new system would be limited to codes U+0000 to U+10FFFF,
which is wasteful when storing them as 32-bit values (called UTF-32), but
ensures that every possible code can be represented in the legacy-compatible
UTF-16 encoding, which is backward compatible with UCS-2, but with added
support for surrogate pairs to represent the new codes over U+100000.
No one would store Unicode as 32-bit characters for storage or transmission,
and most of the time even in memory. UTF-16 was the perfect solution and
could be supported by updating internal API processing, without changing
their public interfaces.
The UTF-16 encoding means all characters from the BMP (U+0000 to U+FFFF) take
2 bytes per characters, while non-BMP (> U+FFFF) take 4 bytes.

Many Unix systems were still based on 8-bit character sets though, and UTF-16
is not compatible with ASCII when processed as a stream of bytes. Maybe
there's a way to make Unicode backward compatible with ASCII as well?
ASCII 1967 is still a 7-bit encoding system, what if we used the 128
remaining values of a byte to create a variable-length encoding of Unicode
instead of creating code pages?
If we use the same technique as for MBCS and UTF-16 encodings, using each of
the 128 codes units available after ASCII as a leading byte followed by any
of the 256 values possible for the following trailing byte, we can only
support 32768 more multi-byte values. We need something more complex.
We could simply say a leading byte is always followed by a fixed number of
extra bytes, but to support the whole Unicode range that would require all
values that are not the original 7-bit ASCII to always use 4 bytes… That is
compatible with ASCII, but very wasteful. The clever solution used by UTF-8
is to support a fully-variable-lenth encoding, where values can be 1 byte,
2 bytes, 3 bytes, or 4 bytes depending on how many bits they really need.
For this, the first byte can be ASCII (0x0xxxxxxx), meaning it's a 1-byte
code, it can be a 2-byte leading value in the form of 0x110xxxxx, a 3-byte
leading value in the form of 0x1110xxxx, or a 4-byte leading value, in the
form 0x11110xxx. The remaining values starting with 0x10xxxxxx are reserved
to be used only as following bytes of multi-byte values.

 [4mUTF-8[24m
  [33m_0123456789ABCDEF[m
 [33m0_[90m␀␁␂␃␄␅␆␇␈␉␊␋␌␍␎␏[m ╶╮
 [33m1_[90m␐␑␒␓␔␕␖␗␘␙␚␛␜␝␞␟[m  │
 [33m2_[90m !"#$%&'()*+,-./[m  ├╴ Single-byte codes
 [33m3_[90m0123456789:;<=>?[m  │  (Identical to ASCII)
 [33m4_[90m@ABCDEFGHIJKLMNO[m  │
 [33m5_[90mPQRSTUVWXYZ[\]^_[m  │
 [33m6_[90m`abcdefghijklmno[m  │
 [33m7_[90mpqrstuvwxyz{|}~␡[m ╶╯
 [33m8_[42;39m                [m ╶╮
 [33m9_[42;39;3m  Continuation  [m  ├╴ Continuation bytes
 [33mA_[42;39;3m      byte      [m  │  (of multi-byte sequence)
 [33mB_[42;39m                [m ╶╯
 [33mC_[100;30m␥␥[44;39;3m  2-byte      [m ╶╮
 [33mD_[44;39;3m     leading    [m  ├╴ Leading bytes
 [33mE_[45;39;3m 3-byte leading [m  │  (of multi-byte sequence)
 [33mF_[46;39;3m 4-by[100;30mte leading [m ╶╯

This clever arrangement can represent all Unicode code points (U+0000 to
U+10FFFF), never confuse applications and libraries expecting plain ASCII,
as multi-byte sequences never use a value that could be interpreted as ASCII,
and makes it possible when reading a byte to immediately know if it is ASCII,
a leading byte, or a continuation byte, so even if a text stream is missing
its beginning, the system can resynchronize as soon as the next code point
starts.

This encoding means plain ASCII takes 1 byte per character, most western,
Hebrew and Arabic characters take 2 bytes, most asian and historic languages
take 3 bytes per character, and non-BMP characters (> U+FFFF) take 4 bytes
per character. So it is very efficient at storing and transmitting files that
contains mostly ASCII, such as HTML files because of all their markup using
ASCII, but is less efficient than UTF-16 for most asian text (CJK).
It has another benefit over UTF-16, since it is a sequence of 8-bit values,
the bytes order is not dependent on the endianness of the system. Once
numbers require more than one byte, they can be stored and transmitted with
the most significant byte first (big-endian), or the least significant byte
first (little-endian), leading to incompatibilities or extra conversions to
handle the two possible arrangements of UTF-16 as UTF-16BE and UTF-16LE.
This makes UTF-8 the ideal choice for cross-systems storage and transmission.


[2;3m]8;;http://www.phm.lu/Philippe Majerus]8;;, July 2021, last updated June 23, 2025 [22;23m

